{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_vectors = pd.read_csv(\"/home/jtvansch/share/embeddings/cn_ce_100_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>...</th>\n",
       "      <th>e41</th>\n",
       "      <th>e42</th>\n",
       "      <th>e43</th>\n",
       "      <th>e44</th>\n",
       "      <th>e45</th>\n",
       "      <th>e46</th>\n",
       "      <th>e47</th>\n",
       "      <th>e48</th>\n",
       "      <th>e49</th>\n",
       "      <th>e50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1.467613</td>\n",
       "      <td>-1.237375</td>\n",
       "      <td>-2.981731</td>\n",
       "      <td>-1.501061</td>\n",
       "      <td>1.877658</td>\n",
       "      <td>1.258956</td>\n",
       "      <td>1.002212</td>\n",
       "      <td>-0.583636</td>\n",
       "      <td>1.460757</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.382263</td>\n",
       "      <td>-1.671850</td>\n",
       "      <td>-1.763873</td>\n",
       "      <td>-1.296559</td>\n",
       "      <td>-0.904948</td>\n",
       "      <td>-4.466372</td>\n",
       "      <td>-1.560971</td>\n",
       "      <td>-1.279335</td>\n",
       "      <td>-2.899061</td>\n",
       "      <td>-1.495396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>0.308166</td>\n",
       "      <td>-0.688937</td>\n",
       "      <td>-0.152841</td>\n",
       "      <td>0.313727</td>\n",
       "      <td>0.174215</td>\n",
       "      <td>-1.087018</td>\n",
       "      <td>-0.683475</td>\n",
       "      <td>0.485497</td>\n",
       "      <td>-2.590817</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.568199</td>\n",
       "      <td>-1.174421</td>\n",
       "      <td>-1.874193</td>\n",
       "      <td>-1.151193</td>\n",
       "      <td>-4.320134</td>\n",
       "      <td>-1.265643</td>\n",
       "      <td>-6.452040</td>\n",
       "      <td>-3.188478</td>\n",
       "      <td>-1.543746</td>\n",
       "      <td>-4.840497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaa</td>\n",
       "      <td>0.185322</td>\n",
       "      <td>1.004584</td>\n",
       "      <td>0.421218</td>\n",
       "      <td>1.123356</td>\n",
       "      <td>-1.478954</td>\n",
       "      <td>-0.024646</td>\n",
       "      <td>-0.409731</td>\n",
       "      <td>0.797623</td>\n",
       "      <td>-1.180365</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.164749</td>\n",
       "      <td>-1.978481</td>\n",
       "      <td>-2.475305</td>\n",
       "      <td>-2.162855</td>\n",
       "      <td>-2.087740</td>\n",
       "      <td>-2.361140</td>\n",
       "      <td>-2.221941</td>\n",
       "      <td>-2.422956</td>\n",
       "      <td>-2.294493</td>\n",
       "      <td>-2.404330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaahhhhhssssss</td>\n",
       "      <td>0.756566</td>\n",
       "      <td>-1.943488</td>\n",
       "      <td>-0.472578</td>\n",
       "      <td>-0.902454</td>\n",
       "      <td>0.581884</td>\n",
       "      <td>-0.269419</td>\n",
       "      <td>-0.503251</td>\n",
       "      <td>0.711363</td>\n",
       "      <td>-0.168953</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.844517</td>\n",
       "      <td>-1.602908</td>\n",
       "      <td>-1.765803</td>\n",
       "      <td>-1.719397</td>\n",
       "      <td>-1.719696</td>\n",
       "      <td>-1.672052</td>\n",
       "      <td>-7.982901</td>\n",
       "      <td>-1.790639</td>\n",
       "      <td>-1.671304</td>\n",
       "      <td>-4.018158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaannnddd</td>\n",
       "      <td>-0.484201</td>\n",
       "      <td>-1.011751</td>\n",
       "      <td>1.582895</td>\n",
       "      <td>-0.642398</td>\n",
       "      <td>-0.182506</td>\n",
       "      <td>-2.098517</td>\n",
       "      <td>-0.736944</td>\n",
       "      <td>1.230328</td>\n",
       "      <td>-0.776474</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197775</td>\n",
       "      <td>-2.068716</td>\n",
       "      <td>-2.379195</td>\n",
       "      <td>-2.210611</td>\n",
       "      <td>-2.272923</td>\n",
       "      <td>-2.189353</td>\n",
       "      <td>-2.306276</td>\n",
       "      <td>-2.514430</td>\n",
       "      <td>-2.229361</td>\n",
       "      <td>-2.437102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31289</th>\n",
       "      <td>θbloom</td>\n",
       "      <td>0.131610</td>\n",
       "      <td>1.122219</td>\n",
       "      <td>-2.413387</td>\n",
       "      <td>0.804525</td>\n",
       "      <td>0.579766</td>\n",
       "      <td>1.105433</td>\n",
       "      <td>-1.029753</td>\n",
       "      <td>0.095673</td>\n",
       "      <td>-1.318471</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.153842</td>\n",
       "      <td>-2.073723</td>\n",
       "      <td>-2.334558</td>\n",
       "      <td>-2.180923</td>\n",
       "      <td>-2.272055</td>\n",
       "      <td>-2.330373</td>\n",
       "      <td>-2.292384</td>\n",
       "      <td>-2.558641</td>\n",
       "      <td>-2.194699</td>\n",
       "      <td>-2.444917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31290</th>\n",
       "      <td>θdiff</td>\n",
       "      <td>1.194438</td>\n",
       "      <td>-0.025920</td>\n",
       "      <td>-0.305598</td>\n",
       "      <td>-0.844286</td>\n",
       "      <td>0.213569</td>\n",
       "      <td>0.629932</td>\n",
       "      <td>1.678933</td>\n",
       "      <td>-0.346384</td>\n",
       "      <td>1.660299</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.081275</td>\n",
       "      <td>-1.904265</td>\n",
       "      <td>-1.719283</td>\n",
       "      <td>-1.994834</td>\n",
       "      <td>-3.524750</td>\n",
       "      <td>-1.965578</td>\n",
       "      <td>-1.666395</td>\n",
       "      <td>-3.030027</td>\n",
       "      <td>-1.603763</td>\n",
       "      <td>-2.373290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31291</th>\n",
       "      <td>θturb</td>\n",
       "      <td>2.180069</td>\n",
       "      <td>-1.161924</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>-1.552069</td>\n",
       "      <td>1.037211</td>\n",
       "      <td>0.045158</td>\n",
       "      <td>-1.167034</td>\n",
       "      <td>0.274082</td>\n",
       "      <td>-1.811515</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.734826</td>\n",
       "      <td>-2.252282</td>\n",
       "      <td>-2.191794</td>\n",
       "      <td>-2.336969</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-3.080508</td>\n",
       "      <td>-1.903692</td>\n",
       "      <td>-2.214810</td>\n",
       "      <td>-2.111354</td>\n",
       "      <td>-1.810452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31292</th>\n",
       "      <td>аɩѕο</td>\n",
       "      <td>0.357447</td>\n",
       "      <td>1.642604</td>\n",
       "      <td>-0.681744</td>\n",
       "      <td>-0.063346</td>\n",
       "      <td>-0.119409</td>\n",
       "      <td>-0.292735</td>\n",
       "      <td>0.544450</td>\n",
       "      <td>-1.135131</td>\n",
       "      <td>1.047704</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.216517</td>\n",
       "      <td>-1.993241</td>\n",
       "      <td>-2.332905</td>\n",
       "      <td>-2.164743</td>\n",
       "      <td>-2.091177</td>\n",
       "      <td>-2.349440</td>\n",
       "      <td>-2.172904</td>\n",
       "      <td>-2.482840</td>\n",
       "      <td>-2.188882</td>\n",
       "      <td>-2.569129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31293</th>\n",
       "      <td>ԁаmаɡеѕ</td>\n",
       "      <td>1.260223</td>\n",
       "      <td>-0.826148</td>\n",
       "      <td>-1.036656</td>\n",
       "      <td>1.189416</td>\n",
       "      <td>-1.419731</td>\n",
       "      <td>-0.328194</td>\n",
       "      <td>0.397294</td>\n",
       "      <td>-0.181017</td>\n",
       "      <td>1.242022</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.022909</td>\n",
       "      <td>-1.908086</td>\n",
       "      <td>-2.494730</td>\n",
       "      <td>-2.075036</td>\n",
       "      <td>-1.976173</td>\n",
       "      <td>-1.956814</td>\n",
       "      <td>-2.076413</td>\n",
       "      <td>-2.173246</td>\n",
       "      <td>-3.718092</td>\n",
       "      <td>-7.541735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31294 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                token        c1        c2        c3        c4        c5  \\\n",
       "0                   a  1.467613 -1.237375 -2.981731 -1.501061  1.877658   \n",
       "1                  aa  0.308166 -0.688937 -0.152841  0.313727  0.174215   \n",
       "2                 aaa  0.185322  1.004584  0.421218  1.123356 -1.478954   \n",
       "3      aaahhhhhssssss  0.756566 -1.943488 -0.472578 -0.902454  0.581884   \n",
       "4           aaannnddd -0.484201 -1.011751  1.582895 -0.642398 -0.182506   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "31289          θbloom  0.131610  1.122219 -2.413387  0.804525  0.579766   \n",
       "31290           θdiff  1.194438 -0.025920 -0.305598 -0.844286  0.213569   \n",
       "31291           θturb  2.180069 -1.161924  0.945274 -1.552069  1.037211   \n",
       "31292            аɩѕο  0.357447  1.642604 -0.681744 -0.063346 -0.119409   \n",
       "31293         ԁаmаɡеѕ  1.260223 -0.826148 -1.036656  1.189416 -1.419731   \n",
       "\n",
       "             c6        c7        c8        c9  ...       e41       e42  \\\n",
       "0      1.258956  1.002212 -0.583636  1.460757  ... -3.382263 -1.671850   \n",
       "1     -1.087018 -0.683475  0.485497 -2.590817  ... -1.568199 -1.174421   \n",
       "2     -0.024646 -0.409731  0.797623 -1.180365  ... -2.164749 -1.978481   \n",
       "3     -0.269419 -0.503251  0.711363 -0.168953  ... -1.844517 -1.602908   \n",
       "4     -2.098517 -0.736944  1.230328 -0.776474  ... -2.197775 -2.068716   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "31289  1.105433 -1.029753  0.095673 -1.318471  ... -2.153842 -2.073723   \n",
       "31290  0.629932  1.678933 -0.346384  1.660299  ... -3.081275 -1.904265   \n",
       "31291  0.045158 -1.167034  0.274082 -1.811515  ... -2.734826 -2.252282   \n",
       "31292 -0.292735  0.544450 -1.135131  1.047704  ... -2.216517 -1.993241   \n",
       "31293 -0.328194  0.397294 -0.181017  1.242022  ... -2.022909 -1.908086   \n",
       "\n",
       "            e43       e44       e45       e46       e47       e48       e49  \\\n",
       "0     -1.763873 -1.296559 -0.904948 -4.466372 -1.560971 -1.279335 -2.899061   \n",
       "1     -1.874193 -1.151193 -4.320134 -1.265643 -6.452040 -3.188478 -1.543746   \n",
       "2     -2.475305 -2.162855 -2.087740 -2.361140 -2.221941 -2.422956 -2.294493   \n",
       "3     -1.765803 -1.719397 -1.719696 -1.672052 -7.982901 -1.790639 -1.671304   \n",
       "4     -2.379195 -2.210611 -2.272923 -2.189353 -2.306276 -2.514430 -2.229361   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "31289 -2.334558 -2.180923 -2.272055 -2.330373 -2.292384 -2.558641 -2.194699   \n",
       "31290 -1.719283 -1.994834 -3.524750 -1.965578 -1.666395 -3.030027 -1.603763   \n",
       "31291 -2.191794 -2.336969 -1.921844 -3.080508 -1.903692 -2.214810 -2.111354   \n",
       "31292 -2.332905 -2.164743 -2.091177 -2.349440 -2.172904 -2.482840 -2.188882   \n",
       "31293 -2.494730 -2.075036 -1.976173 -1.956814 -2.076413 -2.173246 -3.718092   \n",
       "\n",
       "            e50  \n",
       "0     -1.495396  \n",
       "1     -4.840497  \n",
       "2     -2.404330  \n",
       "3     -4.018158  \n",
       "4     -2.437102  \n",
       "...         ...  \n",
       "31289 -2.444917  \n",
       "31290 -2.373290  \n",
       "31291 -1.810452  \n",
       "31292 -2.569129  \n",
       "31293 -7.541735  \n",
       "\n",
       "[31294 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"train.pkl\")\n",
    "test = pd.read_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['token']\n",
    "train_y = train['label']\n",
    "train_id = train['sen_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test['token']\n",
    "test_y = test['label']\n",
    "test_id = test['sen_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([item for sublist in train_x for item in sublist] + [item for sublist in test_x for item in sublist])\n",
    "tag = ['O', 'C', 'E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-192-9fb74df96829>:1: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  vectors =cn_vectors.set_index('token').T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "vectors =cn_vectors.set_index('token').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors['arrows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_300 = np.array([0] * 300)\n",
    "zero_301 = np.concatenate((np.array([1]), zero_300))\n",
    "\n",
    "zero_100 = np.array([0] * 100)\n",
    "zero_101 = np.concatenate((np.array([1]), zero_100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "\n",
    "not_present = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocab:\n",
    "    try:\n",
    "        vec = w2v_model[i]\n",
    "        vec1 = np.concatenate((np.array([0]), vec))\n",
    "        word2idx[i]= vec1\n",
    "        \n",
    "    except:\n",
    "        word2idx[i]= zero_301\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocab:\n",
    "    try:\n",
    "        vec = vectors[i]\n",
    "        vec1 = np.concatenate((np.array([0]), vec))\n",
    "        word2idx[i]= np.concatenate((word2idx[i], vec1))\n",
    "    except:\n",
    "        word2idx[i]= np.concatenate((word2idx[i], zero_101))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00, -5.83496094e-02,  2.92968750e-01,  2.04101562e-01,\n",
       "       -3.47656250e-01, -4.22363281e-02, -1.76757812e-01,  2.08007812e-01,\n",
       "       -7.56835938e-03,  2.83203125e-01, -1.85546875e-01,  2.71484375e-01,\n",
       "       -1.69921875e-01,  4.83398438e-02, -1.74804688e-01, -9.52148438e-02,\n",
       "        2.62451172e-02, -1.73828125e-01, -1.44531250e-01, -3.66210938e-02,\n",
       "       -6.25000000e-02,  5.00488281e-02,  5.78613281e-02, -4.63867188e-02,\n",
       "       -2.05078125e-02,  1.33789062e-01, -2.55859375e-01, -1.36718750e-01,\n",
       "        1.15234375e-01,  9.17968750e-02, -3.51562500e-01, -2.31445312e-01,\n",
       "       -6.29882812e-02, -1.81640625e-01,  1.79687500e-01, -1.80664062e-01,\n",
       "       -2.50000000e-01,  7.47070312e-02,  3.00781250e-01,  1.64062500e-01,\n",
       "       -2.77343750e-01, -2.51953125e-01,  1.46484375e-01,  2.09960938e-01,\n",
       "       -1.88476562e-01, -4.49218750e-02, -2.08984375e-01, -1.01318359e-02,\n",
       "        2.99072266e-02, -1.48437500e-01,  2.73437500e-01,  4.05273438e-02,\n",
       "       -2.92968750e-01,  9.52148438e-02, -1.36718750e-01, -2.37304688e-01,\n",
       "       -2.27050781e-02, -1.70898438e-01,  3.83300781e-02,  1.54296875e-01,\n",
       "        3.17382812e-02,  1.04980469e-01, -1.96289062e-01,  2.25830078e-02,\n",
       "       -3.06640625e-01, -2.05078125e-01,  5.34667969e-02,  1.54296875e-01,\n",
       "        4.32128906e-02, -1.39648438e-01, -1.28906250e-01, -1.26953125e-01,\n",
       "       -6.88476562e-02,  1.35742188e-01, -2.01171875e-01, -2.67578125e-01,\n",
       "       -8.00781250e-02, -2.18750000e-01, -6.39648438e-02,  4.17968750e-01,\n",
       "       -1.68945312e-01,  1.52343750e-01,  2.96630859e-02, -1.79687500e-01,\n",
       "       -2.04101562e-01, -2.85156250e-01,  1.97265625e-01,  1.17187500e-01,\n",
       "       -2.38281250e-01,  2.77343750e-01, -1.44531250e-01, -2.96875000e-01,\n",
       "        2.91015625e-01, -9.66796875e-02,  2.79296875e-01, -3.33984375e-01,\n",
       "       -1.25976562e-01, -8.00781250e-02, -1.54296875e-01,  2.39257812e-01,\n",
       "        2.92968750e-03,  1.04492188e-01,  6.78710938e-02,  3.44238281e-02,\n",
       "       -3.16406250e-01, -5.85937500e-02,  2.34375000e-01, -1.04492188e-01,\n",
       "       -1.30859375e-01,  2.18750000e-01,  1.38671875e-01,  2.17773438e-01,\n",
       "       -1.19628906e-01,  2.94189453e-02, -2.51464844e-02,  2.70996094e-02,\n",
       "       -3.41796875e-01,  2.28515625e-01,  2.30468750e-01, -5.03906250e-01,\n",
       "       -8.64257812e-02,  2.08007812e-01,  3.82812500e-01, -1.85546875e-01,\n",
       "        2.02148438e-01, -1.76757812e-01,  5.00488281e-02,  7.76367188e-02,\n",
       "        3.44238281e-02, -1.05468750e-01,  3.66210938e-02, -1.45507812e-01,\n",
       "        1.15356445e-02, -7.91015625e-02, -2.12890625e-01,  1.09252930e-02,\n",
       "       -2.96875000e-01, -6.00585938e-02, -4.88281250e-02,  5.12695312e-02,\n",
       "        5.11718750e-01, -2.67578125e-01, -2.77343750e-01,  7.66601562e-02,\n",
       "       -9.71679688e-02, -1.77734375e-01, -2.51953125e-01, -7.22656250e-02,\n",
       "        4.78515625e-01, -1.14257812e-01, -3.63281250e-01,  2.36328125e-01,\n",
       "       -4.37500000e-01,  1.18652344e-01,  4.08203125e-01, -1.60156250e-01,\n",
       "       -2.00195312e-01,  2.71484375e-01, -7.66601562e-02,  1.33789062e-01,\n",
       "        7.50732422e-03, -9.86328125e-02, -1.15722656e-01, -1.01562500e-01,\n",
       "        1.05590820e-02, -3.56445312e-02, -1.94335938e-01,  1.17675781e-01,\n",
       "       -1.47460938e-01,  2.91748047e-02,  3.59375000e-01, -2.96875000e-01,\n",
       "       -6.88476562e-02,  2.10571289e-03,  6.07910156e-02, -5.22460938e-02,\n",
       "       -1.94335938e-01,  3.12500000e-01,  1.37695312e-01, -1.46484375e-01,\n",
       "        4.72656250e-01, -2.94921875e-01, -2.96875000e-01, -5.15747070e-03,\n",
       "       -9.96093750e-02,  3.37890625e-01, -8.83789062e-02, -5.20019531e-02,\n",
       "        5.02929688e-02,  2.01225281e-04,  1.06933594e-01, -2.25585938e-01,\n",
       "        1.10839844e-01, -1.05468750e-01, -9.27734375e-02,  1.59179688e-01,\n",
       "       -1.22558594e-01, -1.55273438e-01,  1.28906250e-01,  3.54003906e-02,\n",
       "       -4.04296875e-01,  2.22167969e-02,  1.66015625e-01, -5.46875000e-02,\n",
       "        1.08032227e-02,  8.74023438e-02, -1.81884766e-02,  1.53320312e-01,\n",
       "       -1.32812500e-01, -2.19726562e-02,  3.14453125e-01, -2.71484375e-01,\n",
       "        1.63085938e-01,  3.16406250e-01, -2.19726562e-01, -1.45507812e-01,\n",
       "        2.91748047e-02, -1.91406250e-01, -4.08203125e-01, -1.64062500e-01,\n",
       "        4.27734375e-01,  1.18652344e-01,  1.50390625e-01,  6.12792969e-02,\n",
       "       -1.73339844e-02,  1.55273438e-01, -1.28906250e-01, -5.18798828e-03,\n",
       "        5.42968750e-01,  1.63085938e-01,  3.63281250e-01,  1.36718750e-01,\n",
       "        7.47070312e-02, -3.24218750e-01, -7.27539062e-02, -3.08593750e-01,\n",
       "        1.62353516e-02,  3.37890625e-01, -3.53515625e-01,  4.08203125e-01,\n",
       "        2.92968750e-01, -3.06640625e-01, -1.59912109e-02, -8.88671875e-02,\n",
       "       -1.02539062e-01,  3.16406250e-01, -1.88476562e-01, -2.17773438e-01,\n",
       "        2.41699219e-02, -7.12890625e-02,  2.71484375e-01, -1.41601562e-01,\n",
       "       -2.80761719e-02, -8.69140625e-02,  3.90625000e-01,  1.04492188e-01,\n",
       "       -1.70898438e-01,  5.39550781e-02,  3.01513672e-02, -2.69531250e-01,\n",
       "        1.66992188e-01, -3.07617188e-02,  1.34765625e-01,  1.74804688e-01,\n",
       "        3.10546875e-01, -3.54003906e-03,  5.85937500e-01, -3.08593750e-01,\n",
       "       -1.62109375e-01,  2.11914062e-01,  1.45507812e-01,  1.96533203e-02,\n",
       "       -9.57031250e-02, -2.59765625e-01, -9.03320312e-02,  1.79687500e-01,\n",
       "        1.10839844e-01,  1.10839844e-01, -3.43750000e-01,  3.93066406e-02,\n",
       "       -2.50000000e-01, -2.89306641e-02, -5.68847656e-02,  3.22265625e-01,\n",
       "        1.92382812e-01,  1.89453125e-01, -2.40234375e-01,  5.29785156e-02,\n",
       "        3.80859375e-01,  1.68945312e-01,  1.00097656e-01, -2.57568359e-02,\n",
       "        1.15966797e-02, -4.47265625e-01, -8.97216797e-03, -3.29589844e-02,\n",
       "       -1.66015625e-01, -7.51953125e-02, -2.92968750e-01,  1.89453125e-01,\n",
       "       -1.69921875e-01,  0.00000000e+00,  2.34845757e+00, -1.93476665e+00,\n",
       "        1.11730897e+00,  7.43547201e-01, -1.10065377e+00,  4.65869951e+00,\n",
       "       -1.28281963e+00, -2.25330498e-02, -3.61530155e-01, -2.08763218e+00,\n",
       "       -1.95729494e+00, -9.10270870e-01, -1.27287757e+00, -2.06192374e+00,\n",
       "       -2.48566270e-01,  2.45356536e+00,  1.44099981e-01, -8.28235820e-02,\n",
       "       -1.44211853e+00,  3.08147526e+00, -1.77649748e+00, -3.79936099e+00,\n",
       "        1.43288171e+00, -1.05478954e+00,  5.59313297e-01, -3.20820570e+00,\n",
       "       -2.86324954e+00,  1.36274290e+00, -1.05138373e+00, -8.88123438e-02,\n",
       "        8.09837937e-01,  3.27562714e+00,  1.84081686e+00, -1.23729847e-01,\n",
       "       -3.19970417e+00, -4.23339128e+00, -1.13452351e+00,  8.57126474e-01,\n",
       "       -2.45900750e+00, -3.32988381e-01, -3.79206085e+00, -1.00225365e+00,\n",
       "        3.02258164e-01,  1.56201291e+00, -2.52100372e+00, -2.88584977e-01,\n",
       "       -7.92491555e-01, -6.48450255e-01,  2.78544605e-01, -1.50207198e+00,\n",
       "       -1.80426240e+00, -2.01836967e+00, -3.15945101e+00, -2.61798835e+00,\n",
       "       -1.81493878e+00, -2.15078139e+00, -1.45947397e+00, -1.91482830e+00,\n",
       "       -1.66908717e+00, -1.73646188e+00, -2.96441221e+00, -1.60285997e+00,\n",
       "       -2.18197107e+00, -2.28030467e+00, -2.02077937e+00, -1.57698011e+00,\n",
       "       -2.58504748e+00, -2.26738119e+00, -2.16683221e+00, -1.52115047e+00,\n",
       "       -3.84893274e+00, -3.79074645e+00, -1.96940041e+00, -2.88497972e+00,\n",
       "       -2.79255843e+00, -1.48090553e+00, -2.27676916e+00, -2.49438047e+00,\n",
       "       -1.79562628e+00, -2.83554029e+00, -1.59054911e+00, -1.65753460e+00,\n",
       "       -2.03407836e+00, -2.31055212e+00, -4.14940357e+00, -2.06971002e+00,\n",
       "       -1.92754054e+00, -1.87337303e+00, -1.88238287e+00, -2.53996706e+00,\n",
       "       -2.49421859e+00, -1.64618933e+00, -1.76931608e+00, -2.85966969e+00,\n",
       "       -1.56418872e+00, -1.57430446e+00, -2.66279316e+00, -3.27125859e+00,\n",
       "       -1.47862077e+00, -2.64220166e+00])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5177"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx['guitar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tag)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_idx):\n",
    "    \n",
    "    idxs = [to_idx[w] for w in seq]\n",
    "    idxs = np.array(idxs)\n",
    "    print(idxs.dtype)\n",
    "    output_idxs = torch.from_numpy(idxs)\n",
    "    output_idxs = output_idxs.type(torch.FloatTensor).cuda()\n",
    "    \n",
    "    return output_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e624b1bf0>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #LAYER1 : EMBEDDING LAYER\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #LAYER2 : LSTM\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        #LAYER3 : DENSE\n",
    "        # the linear layer that maps the hidden state output dimension to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        \n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        #return(torch.randn(2, 1, self.hidden_dim).cuda(),\n",
    "        #    torch.randn(2, 1, self.hidden_dim).cuda())\n",
    "        return (torch.zeros(2, 1, self.hidden_dim).cuda(),\n",
    "                torch.zeros(2, 1, self.hidden_dim).cuda())\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        embeds = sentence\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm((embeds.view(len(sentence), 1, -1)), self.hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        \n",
    "        # get the scores for the most likely tag for a word\n",
    "        tag_scores = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(3)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding dimension defines the size of our word vectors\n",
    "# for our simple vocabulary and training set, we will keep these small\n",
    "EMBEDDING_DIM = 402\n",
    "HIDDEN_DIM = 120\n",
    "\n",
    "# instantiate our model\n",
    "model1 = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx))\n",
    "model1.cuda()\n",
    "\n",
    "# define our loss and optimizer\n",
    "loss_function =  nn.CrossEntropyLoss()  #nn.NLLLoss()   \n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.1)       ##SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [i, used, to, get, terrible, headaches, from, ...\n",
       "1      [peters, s, pain, and, symptoms, were, caused,...\n",
       "2      [in, the, years, since, the, adm, scandal, of,...\n",
       "3      [a, first, revolution, was, triggered, by, the...\n",
       "4      [when, they, took, the, floor, against, a, tea...\n",
       "                             ...                        \n",
       "797    [using, solar, electricity, instead, of, conve...\n",
       "798    [fatigue, corrosion, and, stress, corrosion, a...\n",
       "799    [the, electromagnetic, em, radiation, from, th...\n",
       "800    [it, gets, a, little, bit, chicken, first, or,...\n",
       "801    [the, microphone, converts, sound, into, an, e...\n",
       "Name: token, Length: 802, dtype: object"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [many, adults, retain, scars, from, acne, brea...\n",
       "1      [we, estimate, a, wind, speed, associated, wit...\n",
       "2      [outbreaks, caused, by, the, oral, vaccine, s,...\n",
       "3      [production, and, investigation, of, such, a, ...\n",
       "4      [the, drugs, he, sold, had, caused, the, overd...\n",
       "                             ...                        \n",
       "196    [the, real, possibility, of, total, engulfment...\n",
       "197    [we, find, evidence, that, ernst, suffered, a,...\n",
       "198    [the, influx, caused, a, further, drain, on, t...\n",
       "199    [ngu, nongonococcal, urethritis, is, an, infec...\n",
       "200    [eye, discomfort, from, this, staring, effect,...\n",
       "Name: token, Length: 201, dtype: object"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "training_input_data = []\n",
    "for i in range(len(train_x)):\n",
    "    sen = train_x[i]\n",
    "    tag = train_y[i]\n",
    "    sen_t = prepare_sequence(sen, word2idx)\n",
    "    tag_t = prepare_sequence(tag, tag2idx)\n",
    "    training_input_data.append((sen_t, tag_t))\n",
    "    \n",
    "    \n",
    "testing_input_data = []\n",
    "for i in range(len(test_x)):\n",
    "    sen = test_x[i]\n",
    "    tag = test_y[i]\n",
    "    sen_t = prepare_sequence(sen, word2idx)\n",
    "    tag_t = prepare_sequence(tag, tag2idx)\n",
    "    testing_input_data.append((sen_t, tag_t))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_input_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.36747\n",
      "--- 4.530977487564087 seconds ---\n",
      "Epoch: 2, loss: 0.29355\n",
      "--- 9.030852556228638 seconds ---\n",
      "Epoch: 3, loss: 0.25651\n",
      "--- 13.525933027267456 seconds ---\n",
      "Epoch: 4, loss: 0.22290\n",
      "--- 18.027941465377808 seconds ---\n",
      "Epoch: 5, loss: 0.18905\n",
      "--- 22.530105352401733 seconds ---\n",
      "Epoch: 6, loss: 0.15922\n",
      "--- 27.038546085357666 seconds ---\n",
      "Epoch: 7, loss: 0.13375\n",
      "--- 31.540041208267212 seconds ---\n",
      "Epoch: 8, loss: 0.11448\n",
      "--- 36.03359866142273 seconds ---\n",
      "Epoch: 9, loss: 0.09530\n",
      "--- 40.528313875198364 seconds ---\n",
      "Epoch: 10, loss: 0.08627\n",
      "--- 45.02537822723389 seconds ---\n",
      "Epoch: 11, loss: 0.07296\n",
      "--- 49.520262241363525 seconds ---\n",
      "Epoch: 12, loss: 0.06564\n",
      "--- 54.01691770553589 seconds ---\n",
      "Epoch: 13, loss: 0.05167\n",
      "--- 58.51698303222656 seconds ---\n",
      "Epoch: 14, loss: 0.04991\n",
      "--- 63.02384352684021 seconds ---\n",
      "Epoch: 15, loss: 0.04212\n",
      "--- 67.57584190368652 seconds ---\n",
      "Total time taken : \n",
      "--- 67.57665228843689 seconds ---\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "score_save = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    count=0\n",
    "    \n",
    "    for sentence, tags in training_input_data:\n",
    "        if len(sentence)>0:\n",
    "            count+=1\n",
    "            \n",
    "            model1.zero_grad()\n",
    "\n",
    "            # zero the hidden state of the LSTM, this detaches it from its history\n",
    "            model1.hidden = model1.init_hidden()\n",
    "\n",
    "            \n",
    "            # forward \n",
    "           \n",
    "            tags = tags.type(torch.LongTensor).cuda()\n",
    "            \n",
    "            tag_scores = model1(sentence)\n",
    "            \n",
    "            # Loss\n",
    "            tag_scores = tag_scores.type(torch.FloatTensor).cuda()\n",
    "            #targets = targets.type(torch.LongTensor).cuda()\n",
    "            if epoch==n_epochs-1:\n",
    "                score_save.append(tag_scores.tolist())\n",
    "           \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            #print(loss)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # update the model parameters with optimizer.step()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(train_x)))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"Total time taken : \")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for training data\n",
    "\n",
    "pred_tag = []\n",
    "actual_tag = []\n",
    "for sen, tag in training_input_data:\n",
    "    tag_scores = model1(sen)\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "    pred = predicted_tags.cpu().tolist()\n",
    "    pred = [idx2tag[idx] for idx in pred]\n",
    "    pred_tag.append(pred)\n",
    "    tag = tag.cpu().tolist()\n",
    "    act = [idx2tag[idx] for idx in tag]\n",
    "    actual_tag.append(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_f = [item for sublist in pred_tag for item in sublist]\n",
    "actual_tag_f = [item for sublist in actual_tag for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.88      0.70      0.78       890\n",
      "           C       0.96      0.65      0.77       887\n",
      "           E       0.96      0.99      0.98     13185\n",
      "\n",
      "    accuracy                           0.96     14962\n",
      "   macro avg       0.93      0.78      0.84     14962\n",
      "weighted avg       0.96      0.96      0.95     14962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['O', 'C', 'E']\n",
    "\n",
    "print(classification_report(actual_tag_f, pred_tag_f,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for testing data\n",
    "\n",
    "pred_tag = []\n",
    "actual_tag = []\n",
    "for sen, tag in testing_input_data:\n",
    "    tag_scores = model1(sen)\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "    pred = predicted_tags.cpu().tolist()\n",
    "    pred = [idx2tag[idx] for idx in pred]\n",
    "    pred_tag.append(pred)\n",
    "    tag = tag.cpu().tolist()\n",
    "    act = [idx2tag[idx] for idx in tag]\n",
    "    actual_tag.append(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_f = [item for sublist in pred_tag for item in sublist]\n",
    "actual_tag_f = [item for sublist in actual_tag for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.53      0.42      0.47       224\n",
      "           C       0.74      0.41      0.53       217\n",
      "           E       0.94      0.97      0.95      3516\n",
      "\n",
      "    accuracy                           0.91      3957\n",
      "   macro avg       0.74      0.60      0.65      3957\n",
      "weighted avg       0.90      0.91      0.90      3957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['O', 'C', 'E']\n",
    "\n",
    "print(classification_report(actual_tag_f, pred_tag_f,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_x(sen):\n",
    "    sen = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', sen)\n",
    "    sen = sen.replace('.', ' ')\n",
    "    sen = sen.lower()\n",
    "    sen = sen.split()\n",
    "    return sen\n",
    "\n",
    "def clean_y(rel):\n",
    "    cause = rel[0]\n",
    "    cause = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', cause)\n",
    "    cause = cause.lower().split()\n",
    "    effect = rel[1]\n",
    "    effect = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', effect)\n",
    "    effect = effect.lower().split()\n",
    "\n",
    "    return ((cause, effect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST WITH ANY INPUT SENTENCE\n",
    "\n",
    "\n",
    "def test_with_sen(input_sen):\n",
    "    input_sen = clean_x(input_sen)\n",
    "    \n",
    "    inputs = prepare_sequence(input_sen, word2idx)\n",
    "\n",
    "    tag_scores = model1(inputs)\n",
    "\n",
    "\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "\n",
    "    print(\"Input sentence: \", input_sen)\n",
    "    for i in range(0,len(input_sen)):\n",
    "        t = ''\n",
    "        if predicted_tags[i].item()==0:\n",
    "            t = 'Other'\n",
    "        elif predicted_tags[i].item()==1:\n",
    "            t = 'Cause'\n",
    "        elif predicted_tags[i].item()==2:\n",
    "            t = 'Effect'\n",
    "        #elif predicted_tags[i].item==3:\n",
    "        #    t = 'B-Effect'\n",
    "        #else:\n",
    "        #    t = 'I-Effect'\n",
    "        print(predicted_tags[i].item())\n",
    "        print(input_sen[i] , ': ', t )\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sen = 'Smoking causes cancer'\n",
    "test_with_sen(test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ' '.join(test_x[46])\n",
    "\n",
    "print(a)\n",
    "test_with_sen(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
