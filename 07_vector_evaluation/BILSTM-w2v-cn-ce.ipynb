{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_vectors = pd.read_csv(\"cn_ce_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"train.pkl\")\n",
    "test = pd.read_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['token']\n",
    "train_y = train['label']\n",
    "train_id = train['sen_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test['token']\n",
    "test_y = test['label']\n",
    "test_id = test['sen_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([item for sublist in train_x for item in sublist] + [item for sublist in test_x for item in sublist])\n",
    "tag = ['O', 'C', 'E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-9fb74df96829>:1: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  vectors =cn_vectors.set_index('token').T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "vectors =cn_vectors.set_index('token').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors['arrows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_300 = np.array([0] * 300)\n",
    "zero_301 = np.concatenate((np.array([1]), zero_300))\n",
    "\n",
    "zero_100 = np.array([0] * 100)\n",
    "zero_101 = np.concatenate((np.array([1]), zero_100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "\n",
    "not_present = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocab:\n",
    "    try:\n",
    "        vec = w2v_model[i]\n",
    "        vec1 = np.concatenate((np.array([0]), vec))\n",
    "        word2idx[i]= vec1\n",
    "        \n",
    "    except:\n",
    "        word2idx[i]= zero_301\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocab:\n",
    "    try:\n",
    "        vec = vectors[i]\n",
    "        vec1 = np.concatenate((np.array([0]), vec))\n",
    "        word2idx[i]= np.concatenate((word2idx[i], vec1))\n",
    "    except:\n",
    "        word2idx[i]= np.concatenate((word2idx[i], zero_101))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00, -5.83496094e-02,  2.92968750e-01,  2.04101562e-01,\n",
       "       -3.47656250e-01, -4.22363281e-02, -1.76757812e-01,  2.08007812e-01,\n",
       "       -7.56835938e-03,  2.83203125e-01, -1.85546875e-01,  2.71484375e-01,\n",
       "       -1.69921875e-01,  4.83398438e-02, -1.74804688e-01, -9.52148438e-02,\n",
       "        2.62451172e-02, -1.73828125e-01, -1.44531250e-01, -3.66210938e-02,\n",
       "       -6.25000000e-02,  5.00488281e-02,  5.78613281e-02, -4.63867188e-02,\n",
       "       -2.05078125e-02,  1.33789062e-01, -2.55859375e-01, -1.36718750e-01,\n",
       "        1.15234375e-01,  9.17968750e-02, -3.51562500e-01, -2.31445312e-01,\n",
       "       -6.29882812e-02, -1.81640625e-01,  1.79687500e-01, -1.80664062e-01,\n",
       "       -2.50000000e-01,  7.47070312e-02,  3.00781250e-01,  1.64062500e-01,\n",
       "       -2.77343750e-01, -2.51953125e-01,  1.46484375e-01,  2.09960938e-01,\n",
       "       -1.88476562e-01, -4.49218750e-02, -2.08984375e-01, -1.01318359e-02,\n",
       "        2.99072266e-02, -1.48437500e-01,  2.73437500e-01,  4.05273438e-02,\n",
       "       -2.92968750e-01,  9.52148438e-02, -1.36718750e-01, -2.37304688e-01,\n",
       "       -2.27050781e-02, -1.70898438e-01,  3.83300781e-02,  1.54296875e-01,\n",
       "        3.17382812e-02,  1.04980469e-01, -1.96289062e-01,  2.25830078e-02,\n",
       "       -3.06640625e-01, -2.05078125e-01,  5.34667969e-02,  1.54296875e-01,\n",
       "        4.32128906e-02, -1.39648438e-01, -1.28906250e-01, -1.26953125e-01,\n",
       "       -6.88476562e-02,  1.35742188e-01, -2.01171875e-01, -2.67578125e-01,\n",
       "       -8.00781250e-02, -2.18750000e-01, -6.39648438e-02,  4.17968750e-01,\n",
       "       -1.68945312e-01,  1.52343750e-01,  2.96630859e-02, -1.79687500e-01,\n",
       "       -2.04101562e-01, -2.85156250e-01,  1.97265625e-01,  1.17187500e-01,\n",
       "       -2.38281250e-01,  2.77343750e-01, -1.44531250e-01, -2.96875000e-01,\n",
       "        2.91015625e-01, -9.66796875e-02,  2.79296875e-01, -3.33984375e-01,\n",
       "       -1.25976562e-01, -8.00781250e-02, -1.54296875e-01,  2.39257812e-01,\n",
       "        2.92968750e-03,  1.04492188e-01,  6.78710938e-02,  3.44238281e-02,\n",
       "       -3.16406250e-01, -5.85937500e-02,  2.34375000e-01, -1.04492188e-01,\n",
       "       -1.30859375e-01,  2.18750000e-01,  1.38671875e-01,  2.17773438e-01,\n",
       "       -1.19628906e-01,  2.94189453e-02, -2.51464844e-02,  2.70996094e-02,\n",
       "       -3.41796875e-01,  2.28515625e-01,  2.30468750e-01, -5.03906250e-01,\n",
       "       -8.64257812e-02,  2.08007812e-01,  3.82812500e-01, -1.85546875e-01,\n",
       "        2.02148438e-01, -1.76757812e-01,  5.00488281e-02,  7.76367188e-02,\n",
       "        3.44238281e-02, -1.05468750e-01,  3.66210938e-02, -1.45507812e-01,\n",
       "        1.15356445e-02, -7.91015625e-02, -2.12890625e-01,  1.09252930e-02,\n",
       "       -2.96875000e-01, -6.00585938e-02, -4.88281250e-02,  5.12695312e-02,\n",
       "        5.11718750e-01, -2.67578125e-01, -2.77343750e-01,  7.66601562e-02,\n",
       "       -9.71679688e-02, -1.77734375e-01, -2.51953125e-01, -7.22656250e-02,\n",
       "        4.78515625e-01, -1.14257812e-01, -3.63281250e-01,  2.36328125e-01,\n",
       "       -4.37500000e-01,  1.18652344e-01,  4.08203125e-01, -1.60156250e-01,\n",
       "       -2.00195312e-01,  2.71484375e-01, -7.66601562e-02,  1.33789062e-01,\n",
       "        7.50732422e-03, -9.86328125e-02, -1.15722656e-01, -1.01562500e-01,\n",
       "        1.05590820e-02, -3.56445312e-02, -1.94335938e-01,  1.17675781e-01,\n",
       "       -1.47460938e-01,  2.91748047e-02,  3.59375000e-01, -2.96875000e-01,\n",
       "       -6.88476562e-02,  2.10571289e-03,  6.07910156e-02, -5.22460938e-02,\n",
       "       -1.94335938e-01,  3.12500000e-01,  1.37695312e-01, -1.46484375e-01,\n",
       "        4.72656250e-01, -2.94921875e-01, -2.96875000e-01, -5.15747070e-03,\n",
       "       -9.96093750e-02,  3.37890625e-01, -8.83789062e-02, -5.20019531e-02,\n",
       "        5.02929688e-02,  2.01225281e-04,  1.06933594e-01, -2.25585938e-01,\n",
       "        1.10839844e-01, -1.05468750e-01, -9.27734375e-02,  1.59179688e-01,\n",
       "       -1.22558594e-01, -1.55273438e-01,  1.28906250e-01,  3.54003906e-02,\n",
       "       -4.04296875e-01,  2.22167969e-02,  1.66015625e-01, -5.46875000e-02,\n",
       "        1.08032227e-02,  8.74023438e-02, -1.81884766e-02,  1.53320312e-01,\n",
       "       -1.32812500e-01, -2.19726562e-02,  3.14453125e-01, -2.71484375e-01,\n",
       "        1.63085938e-01,  3.16406250e-01, -2.19726562e-01, -1.45507812e-01,\n",
       "        2.91748047e-02, -1.91406250e-01, -4.08203125e-01, -1.64062500e-01,\n",
       "        4.27734375e-01,  1.18652344e-01,  1.50390625e-01,  6.12792969e-02,\n",
       "       -1.73339844e-02,  1.55273438e-01, -1.28906250e-01, -5.18798828e-03,\n",
       "        5.42968750e-01,  1.63085938e-01,  3.63281250e-01,  1.36718750e-01,\n",
       "        7.47070312e-02, -3.24218750e-01, -7.27539062e-02, -3.08593750e-01,\n",
       "        1.62353516e-02,  3.37890625e-01, -3.53515625e-01,  4.08203125e-01,\n",
       "        2.92968750e-01, -3.06640625e-01, -1.59912109e-02, -8.88671875e-02,\n",
       "       -1.02539062e-01,  3.16406250e-01, -1.88476562e-01, -2.17773438e-01,\n",
       "        2.41699219e-02, -7.12890625e-02,  2.71484375e-01, -1.41601562e-01,\n",
       "       -2.80761719e-02, -8.69140625e-02,  3.90625000e-01,  1.04492188e-01,\n",
       "       -1.70898438e-01,  5.39550781e-02,  3.01513672e-02, -2.69531250e-01,\n",
       "        1.66992188e-01, -3.07617188e-02,  1.34765625e-01,  1.74804688e-01,\n",
       "        3.10546875e-01, -3.54003906e-03,  5.85937500e-01, -3.08593750e-01,\n",
       "       -1.62109375e-01,  2.11914062e-01,  1.45507812e-01,  1.96533203e-02,\n",
       "       -9.57031250e-02, -2.59765625e-01, -9.03320312e-02,  1.79687500e-01,\n",
       "        1.10839844e-01,  1.10839844e-01, -3.43750000e-01,  3.93066406e-02,\n",
       "       -2.50000000e-01, -2.89306641e-02, -5.68847656e-02,  3.22265625e-01,\n",
       "        1.92382812e-01,  1.89453125e-01, -2.40234375e-01,  5.29785156e-02,\n",
       "        3.80859375e-01,  1.68945312e-01,  1.00097656e-01, -2.57568359e-02,\n",
       "        1.15966797e-02, -4.47265625e-01, -8.97216797e-03, -3.29589844e-02,\n",
       "       -1.66015625e-01, -7.51953125e-02, -2.92968750e-01,  1.89453125e-01,\n",
       "       -1.69921875e-01,  0.00000000e+00, -1.56489801e+00, -1.54068804e+00,\n",
       "        4.52919692e-01,  1.21354032e+00, -5.83651423e-01,  1.50721520e-02,\n",
       "       -4.25097644e-01, -2.22340710e-02, -1.83534384e+00, -7.91334081e-03,\n",
       "        1.90330863e+00,  8.69145989e-01,  1.06939268e+00, -1.76852536e+00,\n",
       "       -1.96078122e+00,  1.44350863e+00, -2.13315058e+00,  2.16757989e+00,\n",
       "       -2.18305588e+00,  4.86732990e-01, -6.76006258e-01,  7.92059362e-01,\n",
       "       -1.19393021e-01, -2.19388270e+00, -1.20863032e+00,  1.25469542e+00,\n",
       "       -1.15216100e+00,  1.78308129e+00, -1.34113026e+00,  2.00463271e+00,\n",
       "       -1.43811762e+00,  3.62127513e-01, -2.31441617e+00, -1.30208060e-01,\n",
       "        6.55281782e-01, -9.25387263e-01,  1.63231224e-01,  4.08044279e-01,\n",
       "        9.60916638e-01, -2.24281168e+00,  2.25141533e-02,  6.03031337e-01,\n",
       "       -7.44589865e-01,  1.01551688e+00, -2.50919342e+00, -8.19371700e-01,\n",
       "       -7.02505350e-01, -1.29246199e+00,  8.20673883e-01, -3.56853008e-01,\n",
       "       -1.02957654e+00, -9.95078444e-01, -9.33941901e-01, -9.39282596e-01,\n",
       "       -1.09649968e+00, -9.70690370e-01, -1.07752120e+00, -1.14847589e+00,\n",
       "       -1.02166414e+00, -1.11989260e+00, -8.95786226e-01, -1.00735199e+00,\n",
       "       -1.09456873e+00, -9.42427874e-01, -1.05965078e+00, -9.48694289e-01,\n",
       "       -9.46140349e-01, -9.37190115e-01, -1.14272022e+00, -1.12435699e+00,\n",
       "       -1.02862442e+00, -1.05489528e+00, -1.06789303e+00, -8.68413091e-01,\n",
       "       -1.08926690e+00, -1.06720483e+00, -8.91810596e-01, -1.00866413e+00,\n",
       "       -9.45047855e-01, -1.00942528e+00, -9.86718178e-01, -1.07663167e+00,\n",
       "       -1.06651354e+00, -9.93023694e-01, -9.63052928e-01, -9.04161751e-01,\n",
       "       -9.42701757e-01, -9.15944636e-01, -8.69022846e-01, -9.96843874e-01,\n",
       "       -9.18856561e-01, -1.14172506e+00, -9.90834117e-01, -1.10556662e+00,\n",
       "       -1.00494266e+00, -8.60319555e-01, -1.08997416e+00, -9.08366084e-01,\n",
       "       -9.10660803e-01, -1.01375008e+00])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5177"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx['guitar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tag)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_idx):\n",
    "    \n",
    "    idxs = [to_idx[w] for w in seq]\n",
    "    idxs = np.array(idxs)\n",
    "    output_idxs = torch.from_numpy(idxs)\n",
    "    output_idxs = output_idxs.type(torch.FloatTensor).cuda()\n",
    "    \n",
    "    return output_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe6fcfd6bf0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #LAYER1 : EMBEDDING LAYER\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #LAYER2 : LSTM\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        #LAYER3 : DENSE\n",
    "        # the linear layer that maps the hidden state output dimension to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        \n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        return(torch.randn(2, 1, self.hidden_dim).cuda(),\n",
    "            torch.randn(2, 1, self.hidden_dim).cuda())\n",
    "       #return (torch.zeros(2, 1, self.hidden_dim).cuda(),\n",
    "       #    torch.zeros(2, 1, self.hidden_dim).cuda())\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        # LAYER1\n",
    "        #embeds = self.word_embeddings(sentence)\n",
    "        #print(embeds.shape)\n",
    "        \n",
    "        embeds = sentence\n",
    "        #print(embeds.shape)\n",
    "                \n",
    "        lstm_out, self.hidden = self.lstm((embeds.view(len(sentence), 1, -1)), self.hidden)\n",
    "                \n",
    "        # get the scores for the most likely tag for a word\n",
    "        tag_scores = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(3)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding dimension defines the size of our word vectors\n",
    "# for our simple vocabulary and training set, we will keep these small\n",
    "EMBEDDING_DIM = 402\n",
    "HIDDEN_DIM = 120\n",
    "\n",
    "# instantiate our model\n",
    "model1 = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx))\n",
    "model1.cuda()\n",
    "\n",
    "# define our loss and optimizer\n",
    "loss_function =  nn.CrossEntropyLoss()  #nn.NLLLoss()   \n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.1)       ##SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [i, used, to, get, terrible, headaches, from, ...\n",
       "1      [peters, s, pain, and, symptoms, were, caused,...\n",
       "2      [in, the, years, since, the, adm, scandal, of,...\n",
       "3      [a, first, revolution, was, triggered, by, the...\n",
       "4      [when, they, took, the, floor, against, a, tea...\n",
       "                             ...                        \n",
       "797    [using, solar, electricity, instead, of, conve...\n",
       "798    [fatigue, corrosion, and, stress, corrosion, a...\n",
       "799    [the, electromagnetic, em, radiation, from, th...\n",
       "800    [it, gets, a, little, bit, chicken, first, or,...\n",
       "801    [the, microphone, converts, sound, into, an, e...\n",
       "Name: token, Length: 802, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [many, adults, retain, scars, from, acne, brea...\n",
       "1      [we, estimate, a, wind, speed, associated, wit...\n",
       "2      [outbreaks, caused, by, the, oral, vaccine, s,...\n",
       "3      [production, and, investigation, of, such, a, ...\n",
       "4      [the, drugs, he, sold, had, caused, the, overd...\n",
       "                             ...                        \n",
       "196    [the, real, possibility, of, total, engulfment...\n",
       "197    [we, find, evidence, that, ernst, suffered, a,...\n",
       "198    [the, influx, caused, a, further, drain, on, t...\n",
       "199    [ngu, nongonococcal, urethritis, is, an, infec...\n",
       "200    [eye, discomfort, from, this, staring, effect,...\n",
       "Name: token, Length: 201, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_data = []\n",
    "for i in range(len(train_x)):\n",
    "    sen = train_x[i]\n",
    "    tag = train_y[i]\n",
    "    sen_t = prepare_sequence(sen, word2idx)\n",
    "    tag_t = prepare_sequence(tag, tag2idx)\n",
    "    training_input_data.append((sen_t, tag_t))\n",
    "    \n",
    "    \n",
    "testing_input_data = []\n",
    "for i in range(len(test_x)):\n",
    "    sen = test_x[i]\n",
    "    tag = test_y[i]\n",
    "    sen_t = prepare_sequence(sen, word2idx)\n",
    "    tag_t = prepare_sequence(tag, tag2idx)\n",
    "    testing_input_data.append((sen_t, tag_t))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_input_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.39881\n",
      "--- 4.7429563999176025 seconds ---\n",
      "Epoch: 2, loss: 0.29724\n",
      "--- 9.776318073272705 seconds ---\n",
      "Epoch: 3, loss: 0.26176\n",
      "--- 14.646175861358643 seconds ---\n",
      "Epoch: 4, loss: 0.23182\n",
      "--- 19.578099966049194 seconds ---\n",
      "Epoch: 5, loss: 0.20665\n",
      "--- 24.374277591705322 seconds ---\n",
      "Epoch: 6, loss: 0.18529\n",
      "--- 29.394385814666748 seconds ---\n",
      "Epoch: 7, loss: 0.16695\n",
      "--- 34.222697734832764 seconds ---\n",
      "Epoch: 8, loss: 0.14952\n",
      "--- 38.955646276474 seconds ---\n",
      "Epoch: 9, loss: 0.13260\n",
      "--- 43.79750657081604 seconds ---\n",
      "Epoch: 10, loss: 0.12136\n",
      "--- 48.917280197143555 seconds ---\n",
      "Epoch: 11, loss: 0.10530\n",
      "--- 53.79203915596008 seconds ---\n",
      "Epoch: 12, loss: 0.09496\n",
      "--- 58.54454183578491 seconds ---\n",
      "Epoch: 13, loss: 0.08584\n",
      "--- 63.006181478500366 seconds ---\n",
      "Epoch: 14, loss: 0.07327\n",
      "--- 67.46261286735535 seconds ---\n",
      "Epoch: 15, loss: 0.06745\n",
      "--- 72.06460404396057 seconds ---\n",
      "Total time taken : \n",
      "--- 72.06547021865845 seconds ---\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "score_save = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    count=0\n",
    "    \n",
    "    for sentence, tags in training_input_data:\n",
    "        if len(sentence)>0:\n",
    "            count+=1\n",
    "            \n",
    "            model1.zero_grad()\n",
    "\n",
    "            # zero the hidden state of the LSTM, this detaches it from its history\n",
    "            model1.hidden = model1.init_hidden()\n",
    "\n",
    "            # forward \n",
    "           \n",
    "            tags = tags.type(torch.LongTensor).cuda()\n",
    "            \n",
    "            tag_scores = model1(sentence)\n",
    "            \n",
    "            # Loss\n",
    "            tag_scores = tag_scores.type(torch.FloatTensor).cuda()\n",
    "            #targets = targets.type(torch.LongTensor).cuda()\n",
    "            if epoch==n_epochs-1:\n",
    "                score_save.append(tag_scores.tolist())\n",
    "            \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            #print(loss)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # update the model parameters with optimizer.step()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(train_x)))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"Total time taken : \")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for training data\n",
    "\n",
    "pred_tag = []\n",
    "actual_tag = []\n",
    "for sen, tag in training_input_data:\n",
    "    tag_scores = model1(sen)\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "    pred = predicted_tags.cpu().tolist()\n",
    "    pred = [idx2tag[idx] for idx in pred]\n",
    "    pred_tag.append(pred)\n",
    "    tag = tag.cpu().tolist()\n",
    "    act = [idx2tag[idx] for idx in tag]\n",
    "    actual_tag.append(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_f = [item for sublist in pred_tag for item in sublist]\n",
    "actual_tag_f = [item for sublist in actual_tag for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.96      0.66      0.78       890\n",
      "           C       0.83      0.92      0.87       887\n",
      "           E       0.98      0.99      0.98     13185\n",
      "\n",
      "    accuracy                           0.97     14962\n",
      "   macro avg       0.92      0.86      0.88     14962\n",
      "weighted avg       0.97      0.97      0.96     14962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['O', 'C', 'E']\n",
    "\n",
    "print(classification_report(actual_tag_f, pred_tag_f,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for testing data\n",
    "\n",
    "pred_tag = []\n",
    "actual_tag = []\n",
    "for sen, tag in testing_input_data:\n",
    "    tag_scores = model1(sen)\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "    pred = predicted_tags.cpu().tolist()\n",
    "    pred = [idx2tag[idx] for idx in pred]\n",
    "    pred_tag.append(pred)\n",
    "    tag = tag.cpu().tolist()\n",
    "    act = [idx2tag[idx] for idx in tag]\n",
    "    actual_tag.append(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_f = [item for sublist in pred_tag for item in sublist]\n",
    "actual_tag_f = [item for sublist in actual_tag for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.59      0.34      0.44       224\n",
      "           C       0.62      0.70      0.66       217\n",
      "           E       0.95      0.97      0.96      3516\n",
      "\n",
      "    accuracy                           0.92      3957\n",
      "   macro avg       0.72      0.67      0.68      3957\n",
      "weighted avg       0.91      0.92      0.91      3957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['O', 'C', 'E']\n",
    "\n",
    "print(classification_report(actual_tag_f, pred_tag_f,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_x(sen):\n",
    "    sen = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', sen)\n",
    "    sen = sen.replace('.', ' ')\n",
    "    sen = sen.lower()\n",
    "    sen = sen.split()\n",
    "    return sen\n",
    "\n",
    "def clean_y(rel):\n",
    "    cause = rel[0]\n",
    "    cause = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', cause)\n",
    "    cause = cause.lower().split()\n",
    "    effect = rel[1]\n",
    "    effect = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', effect)\n",
    "    effect = effect.lower().split()\n",
    "\n",
    "    return ((cause, effect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST WITH ANY INPUT SENTENCE\n",
    "\n",
    "\n",
    "def test_with_sen(input_sen):\n",
    "    input_sen = clean_x(input_sen)\n",
    "    \n",
    "    inputs = prepare_sequence(input_sen, word2idx)\n",
    "\n",
    "    tag_scores = model1(inputs)\n",
    "\n",
    "\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "\n",
    "    print(\"Input sentence: \", input_sen)\n",
    "    for i in range(0,len(input_sen)):\n",
    "        t = ''\n",
    "        if predicted_tags[i].item()==0:\n",
    "            t = 'Other'\n",
    "        elif predicted_tags[i].item()==1:\n",
    "            t = 'Cause'\n",
    "        elif predicted_tags[i].item()==2:\n",
    "            t = 'Effect'\n",
    "        #elif predicted_tags[i].item==3:\n",
    "        #    t = 'B-Effect'\n",
    "        #else:\n",
    "        #    t = 'I-Effect'\n",
    "        print(predicted_tags[i].item())\n",
    "        print(input_sen[i] , ': ', t )\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:  ['smoking', 'causes', 'cancer']\n",
      "1\n",
      "smoking :  Cause\n",
      "0\n",
      "causes :  Other\n",
      "2\n",
      "cancer :  Effect\n"
     ]
    }
   ],
   "source": [
    "test_sen = 'Smoking causes cancer'\n",
    "test_with_sen(test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are many types and causes of headaches from tension headaches due to stress to migraines triggered by certain foods\n",
      "Input sentence:  ['there', 'are', 'many', 'types', 'and', 'causes', 'of', 'headaches', 'from', 'tension', 'headaches', 'due', 'to', 'stress', 'to', 'migraines', 'triggered', 'by', 'certain', 'foods']\n",
      "0\n",
      "there :  Other\n",
      "0\n",
      "are :  Other\n",
      "0\n",
      "many :  Other\n",
      "0\n",
      "types :  Other\n",
      "0\n",
      "and :  Other\n",
      "0\n",
      "causes :  Other\n",
      "0\n",
      "of :  Other\n",
      "2\n",
      "headaches :  Effect\n",
      "0\n",
      "from :  Other\n",
      "0\n",
      "tension :  Other\n",
      "2\n",
      "headaches :  Effect\n",
      "0\n",
      "due :  Other\n",
      "0\n",
      "to :  Other\n",
      "0\n",
      "stress :  Other\n",
      "0\n",
      "to :  Other\n",
      "0\n",
      "migraines :  Other\n",
      "0\n",
      "triggered :  Other\n",
      "0\n",
      "by :  Other\n",
      "0\n",
      "certain :  Other\n",
      "0\n",
      "foods :  Other\n"
     ]
    }
   ],
   "source": [
    "a = ' '.join(test_x[46])\n",
    "\n",
    "print(a)\n",
    "test_with_sen(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
